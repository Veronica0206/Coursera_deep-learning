{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep L-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"screenshot/11.PNG\" style=\"width:600px;height:350px;\">\n",
    "\n",
    "- Deep neural network notation\n",
    "    - $l=4$ (\\# of layers); \n",
    "    - $n^{[l]}$ (\\# of units in layer $l$): $n^{[0]}=3$, $n^{[1]}=5$, $n^{[2]}=5$, $n^{[3]}=3$, $n^{[4]}=1$\n",
    "    - $a^{[l]}$ (activations in layer $l$): $a^{[l]}=g^{[l]}(z^{[l]})$\n",
    "    - $w^{[l]}$, $b^{[l]}$: weights for $z^{[l]}$\n",
    "<img src=\"screenshot/12.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation in a deep network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "&for\\ l\\ in\\ 1:L:\\\\\n",
    "&\\quad z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}\\\\\n",
    "&\\quad a^{[l-1]}=g^{[l-1]}(z^{[l-1]})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your matrix dimensions right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimension of $w^{[l]}$ is $(n^{[l]},n^{[l-1]})$, can be thought from right to left\n",
    "- Dimension of $b^{[l]}$ is $(n^{[l]},1)$\n",
    "- $dw^{[l]}$ has the same shape as $w^{[l]}$, while $db^{[l]}$ has the same shape as $b^{[l]}$\n",
    "- Dimension of $z^{[l]}$, $A^{[l]}$, $dz^{[l]}$, $dA^{[l]}$ is $(n^{[l]},m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deep representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep NN makes relation with data from simpler to complex. In each layer, it tries to make a relation with the previous layer.\n",
    "<img src=\"screenshot/13.PNG\" style=\"width:600px;height:350px;\">\n",
    "\n",
    "    - Face recognition application: image$\\rightarrow$edges$\\rightarrow$face parts$\\rightarrow$faces$\\rightarrow$desired face\n",
    "    - Audio recognition application: audio$\\rightarrow$low level sound features like \"sss\" or \"bb\"$\\rightarrow$phonemes$\\rightarrow$words$\\rightarrow$sentences\n",
    "    \n",
    "**Neural researchers think that deep neural networks \"thinks\" like brains**\n",
    "\n",
    "    - Circuit theory and deep learning\n",
    "<img src=\"screenshot/14.PNG\" style=\"width:600px;height:350px;\">    \n",
    "\n",
    "**When starting on an application, don't start directly by dozens of hidden layers. Try the simplest solutions (e.g., logistic regression), then try the shallow neural network and so on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks of deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward and backward functions\n",
    "    - forward propagation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&Input\\ a^{[l-1]}\\\\\n",
    "&\\quad z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}\\\\\n",
    "&\\quad a^{[l]}=g^{[l]}(z^{[l]})\\\\\n",
    "&Output\\ a^{[l]}\\\\\n",
    "&Cache\\ z^{[l]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    - backward propagation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&Input\\ da^{[l]}, Caches\\\\\n",
    "&\\quad dz^{[l]}=da^{[l]}*g^{'[l]}(z^{[l]})\\\\\n",
    "&\\quad dw^{[l]}=np.dot(dz^{[l]}, a^{[l-1]})/m\\\\\n",
    "&\\quad db^{[l]}=np.sum(z^{[l]})/m\\\\\n",
    "&\\quad da^{[l-1]}=np.dot(w^{[l]},dz^{[l]})\\\\\n",
    "&Output\\ da^{[l-1]}, dw^{[l]}, db^{[l]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<img src=\"screenshot/15.PNG\" style=\"width:600px;height:350px;\">\n",
    "<img src=\"screenshot/16.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters vs hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main parameters of the NN is `w` and `b`\n",
    "- Hyperparameters (parameters that control the algorithm) are like: learning rate, number of iteration, number of hidden layers `L`, number of hidden units `n`, choice of activation functions\n",
    "- Have to try values of hyperparameters\n",
    "- In the earlier days of ML, the learning rate was often called a parameter, but it is a hyperparameter\n",
    "- On the next course, we will see how to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this have to do with the brain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"screenshot/17.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
