{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to do a logistic regression to nake a binary classifier. For example, to know if the current image contains a cat or not.\n",
    "\n",
    "- Notation: \n",
    "    - $(x,y)$, $x\\in R^{n_{x}}$, $y\\in\\{0,1\\}$\n",
    "    - m training samples: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\dots,(x^{(m)},y^{(m)})\\}$\n",
    "    - $x\\in R^{m*n_{x}}$, $y\\in R^{m*1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An algorithm is used for the classification algorithm of $2$ classes\n",
    "- Equation: $\\hat{y}=\\sigma(w^{T}x+b)$, where $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "    - If $z$ is large, $\\sigma(z)\\approx\\frac{1}{1+0}=1$\n",
    "    - If $z$ is large, $\\sigma(z)\\approx\\frac{1}{1+big\\ \\#}=0$\n",
    "- Parameters: $w$ ($n_{x}$ dimension vector) and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equation: $\\hat{y}=\\sigma(w^{T}x+b)$, where $\\sigma(z)=\\frac{1}{1+e^{-z}}$, given $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\dots,(x^{(m)},y^{(m)})\\}$, want $\\hat{y}^{(i)}\\approx y^{(i)}$\n",
    "- Loss (error) function: the error for a single training example \n",
    "    - Square loss: $L(\\hat{y},y)=\\frac{1}{2}(\\hat{y}-y)^{2}$\n",
    "    - Logistic loss: $L(\\hat{y},y)=-(y\\log\\hat{y}+(1-y)\\log(1-\\hat{y}))$\n",
    "        - If $y=1$, $L(\\hat{y},y)=-\\log\\hat{y}$, want $\\log\\hat{y}$ large, want $\\hat{y}$ large (approach to $1$).\n",
    "        - If $y=0$, $L(\\hat{y},y)=-\\log(1-\\hat{y})$, want $\\log(1-\\hat{y})$ large, want $\\hat{y}$ small (approach to $0$).\n",
    "- Cost function: the average of the loss function of the entire training set.\n",
    "$$\n",
    "J(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}\\bigg(-(y\\log\\hat{y}+(1-y)\\log(1-\\hat{y}))\\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Want to find $(w,b)$ that minimize $J(w,b)$ (convex function (**a single big bowl**))\n",
    "- Repeat \n",
    "$$\n",
    "\\begin{aligned}\n",
    "&w:w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}\\\\\n",
    "&b:b-\\alpha\\frac{\\partial J(w,b)}{\\partial b}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\alpha$ is learning rate. When the slope is negative, then $w$ increases and when the slope is positive, then $w$ decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is the slope and slope is the difference in different points in the function, that's why the derivative is a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A computation graph that organizes the computation **(forward) from left to right**. \n",
    "<img src=\"screenshot/4.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives with a computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A computation graph that organizes derivative (including chain rule) **backward (from right to left)**.\n",
    "<img src=\"screenshot/5.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression recap\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&z=w^{T}x+b\\\\\n",
    "&\\hat{y}=a=\\sigma(z)\\\\\n",
    "&L(a,y)=-(y\\log(a)+(1-y)\\log(1-a))\n",
    "\\end{aligned}\n",
    "$$\n",
    "<img src=\"screenshot/6.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent on m examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J=0$, $dw_{1}=0$, $dw_{2}=0$, $db=0$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&For\\ i=1\\ to\\ m:\\\\\n",
    "&\\quad z^{(i)}=w^{T}x^{(i)}+b\\\\\n",
    "&\\quad a^{(i)}=\\sigma(z^{(i)})\\\\\n",
    "&\\quad J+=-(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\\\\n",
    "&\\quad dz^{(i)}=a^{(i)}-y^{(i)}\\\\\n",
    "&\\quad dw_{1}+=x_{1}^{(i)}dz^{(i)}\\\\\n",
    "&\\quad dw_{2}+=x_{2}^{(i)}dz^{(i)}\\\\\n",
    "&\\quad db+=dz^{(i)}\\\\\n",
    "&J/=m; dw_{1}/=m; dw_{2}/=m; db/=m;\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning shines when the dataset is big. However, for loops will make you wait a lot for a result. That's why we need vectorization to get rid of some of our for loops.\n",
    "- NumPy library `dot` function is using vectorization by default.\n",
    "- The vectorization can be done on CPU or GPU thought the SIMD operation. But it's faster on GPU.\n",
    "- Whenever possible, avoid for loops. \n",
    "- Most of the NumPy library methods are vectorized version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: $x: (n_x, m)$, $y: (n_y, m)$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&z=np.dot(w.T, x) +b\\\\\n",
    "&a=1/(1+np.exp(-z))\\\\\n",
    "&dz=a-y\\\\\n",
    "&dw=np.dot(x,dz.T)/m\\\\\n",
    "&db=dz.sum()/m\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Python and NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In NumPy, $obj.sum(axis = 0)$ sums the columns while $obj.sum(axis = 1)$ sums the rows.\n",
    "- In NumPy, $obj.shape(1,4)$ changes the shape of the matrix by broadcasting the values\n",
    "- Reshape is cheap in calculations so put it everywhere you're not sure about the calculation\n",
    "- Broadcasting works when you do a matrix operation with matrices that don't match for the operation; in this case, NumPy automatically makes the shapes ready for the operation by broadcasting the values. \n",
    "- A general principle of broadcasting. If you have an $(m,n)$ matrix and you add $(+)$ or subtract $(-)$ or multiply $(*)$ or divide $(/)$ with a $(1,n)$ matrix, then this will copy it $m$ times into an $(m,n)$ matrix. The same with if you use those operations with a $(m,1)$ matrix, then this will copy it $n$ times into $(m,n)$ matrix. And then apply the addition, subtraction, and multiplication of division element-wise.  \n",
    "- Some tricks to eliminate all the strange bugs in the code:\n",
    "    - If you didn't specify the shape of a vector, it would take shape of $(m,)$ and the transpose operation won't work. You have to reshape it to $(m,1)$\n",
    "    - Try not to use the rank one matrix a NN.\n",
    "    - Don't hesitate to use `assert(a.shape==(5,1))` to check if your matrix shape is the required one.\n",
    "    - If you've found a rank one matrix try to run reshape on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
