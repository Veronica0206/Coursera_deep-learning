{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to tune our hyperparameters to get the best out of them.\n",
    "- Hyperparameters importance are:\n",
    "    - Learning rate $\\alpha$\n",
    "    - Momentum beta $\\beta$\n",
    "    - Mini-batch size\n",
    "    - \\# of hidden units\n",
    "    - \\# of layers\n",
    "    - Learning rate decay\n",
    "    - Regularization lambda\n",
    "    - Activation function\n",
    "    - Adam $\\beta_{1}$, $\\beta_{2}$ and $\\epsilon$\n",
    "- It's hard to decide which hyperparameter is the most important in a problem. It depends a lot on the problem.\n",
    "- One way to tune is to sample grid with $N$ hyperparameter settings and then try all setting combinations \n",
    "- Try random values: don't use a grid\n",
    "- Use the` Coarse to fine sampling scheme`: When you find some hyperparameter values that give you a better performance--zoom in to a smaller region around these values and sample more densely within this space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an appropriate scale to pick hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose you have a specific range for a hyperparameter from \"a\" to \"b\". It's better to search for the right ones using the logarithmic scale rather than on a linear scale\n",
    "<img src=\"screenshot/13.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning in practice: Pandas vs. Caviar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intuitions about hyperparameter settings from one application area may or may not transfer to a different one.\n",
    "- If you don't have many computational resources, you can use the \"babysitting model\".\n",
    "- Panda approach:\n",
    "    - Day $0$, initialize your parameter as random and then start training\n",
    "    - Watch your learning curve gradually decrease over the day\n",
    "    - And each day you nudge your parameters a little during training\n",
    "- Caviar approach: \n",
    "    - If you have enough computational resources, you can run some models in parallel, and at the end of the day(s) you check the results.\n",
    "\n",
    "<img src=\"screenshot/14.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing activations in a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the rising of deep learning, one of essential ideas has been an algorithm called batch normalization. Batch normalization speeds up learning.\n",
    "\n",
    "<img src=\"screenshot/15.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting batch normalization into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using batch norm in $3$ hidden layers NN\n",
    "<img src=\"screenshot/16.PNG\" style=\"width:600px;height:350px;\">\n",
    "- Working with mini-batches\n",
    "<img src=\"screenshot/17.PNG\" style=\"width:600px;height:350px;\">\n",
    "- Implementing gradient descent\n",
    "<img src=\"screenshot/18.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does batch normalization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first reason is the same reason as to why we normalize $X$\n",
    "- The second reason is that batch normalization reduces the problem of input values shifting\n",
    "- Batch normalization does some regularization:\n",
    "    - Each mini-batch is scaled by the mean/variance computed of that mini-batch\n",
    "    - This adds some noise to the values $Z^{[l]}$ within that mini-batch, so similar to dropout it adds some noise to each hidden layer's activations\n",
    "    - This has a slight regularization effect\n",
    "    - Using a bigger size of the mini-batch, you are reducing noise and therefore regularization effect\n",
    "    - Don't rely on batch normalization as regularization. It's intended for normalization of hidden units, activations and therefore speeding up learning. For regularization use other regularization techniques. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we train a NN with Batch normalization, we compute the mean and the variance of the mini-batch\n",
    "- In testing we might need to process examples one at a time. The mean and the variance of one example won't make sense.\n",
    "- We have to compute an estimated value of mean and variance to use it in testing time\n",
    "- We can use the weighted average across the mini-batches\n",
    "- We will use the estimated values of the mean and variance to test\n",
    "- This method is also sometimes called \"running average\"\n",
    "- In practice most often you will use a deep learning framework and it will contain some default implementation of doing such a thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Softmax layer\n",
    "<img src=\"screenshot/19.PNG\" style=\"width:600px;height:350px;\">\n",
    "- Loss function\n",
    "<img src=\"screenshot/20.PNG\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to choose a deep learning framework\n",
    "    - Ease of programming (development and deployment)\n",
    "    - Running speed\n",
    "    - Truly open (open source with good governance)\n",
    "    - Programming frameworks cannot only shorten your coding time but sometimes also perform optimizations that speed up your code\n",
    "\n",
    "- TensorFlow\n",
    "- Example 1\n",
    "    \n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "w = tf.Variable(0, dtype = tf.float32)                            ## creating a variable w\n",
    "cost = tf.add(tf.add(w ** 2, tf.multiply(-10.0, w)), 25.0)        ## can be written as this - cost = w**2 - 10*w + 25\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "session.run(w)    # Runs the definition of w, if you print this it will print zero\n",
    "session.run(train)\n",
    "\n",
    "print(\"W after one iteration:\", session.run(w))\n",
    "for i in range(1000):\n",
    "    session.run(train)\n",
    "print(\"W after 1000 iterations:\", session.run(w))\n",
    "\n",
    "```\n",
    "\n",
    "- Example 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "coefficients = np.array([[1.], [-10.], [25.]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [3, 1])\n",
    "w = tf.Variable(0, dtype = tf.float32)                 # Creating a variable w\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "session.run(w)                                         # Runs the definition of w, if you print this it will print zero\n",
    "session.run(train, feed_dict = {x: coefficients})\n",
    "\n",
    "print(\"W after one iteration:\", session.run(w))\n",
    "\n",
    "for i in range(1000):\n",
    "    session.run(train, feed_dict = {x: coefficients})\n",
    "\n",
    "print(\"W after 1000 iterations:\", session.run(w))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
